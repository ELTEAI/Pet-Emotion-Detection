import os
import re
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline
)
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from langchain.chains import LLMChain

# â€”â€” å¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼ â€”â€”
os.environ["TRANSFORMERS_OFFLINE"] = "1"

MODEL_PATH = "/app/LLM_Models/Lora_Add_DeepSeek_R1_Distill_Qwen_1_5B"
device = "cuda" if torch.cuda.is_available() else "cpu"

# â€”â€” åŠ è½½ tokenizer â€”â€”
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
    use_fast=True
)

# â€”â€” ä½¿ç”¨ 4bit é‡åŒ–åŠ é€ŸåŠ è½½ â€”â€”
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
    quantization_config=bnb_config,
    device_map="auto"
).eval()

# â€”â€” æ„å»ºå…±äº« pipelineï¼Œæ‰©å¤§ç”Ÿæˆé•¿åº¦ â€”â€” 
text_gen = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    max_new_tokens=1024,       # å°†æ–°ç”Ÿæˆçš„æœ€å¤§ token æ•°æ‰©å¤§åˆ° 512
    max_length=2048,          # æ•´ä½“æœ€å¤§é•¿åº¦ï¼ˆåŒ…æ‹¬è¾“å…¥ï¼‰
    do_sample=True,
    temperature=0.4,
    top_p=0.9,
)

llm = HuggingFacePipeline(pipeline=text_gen)

# â€”â€” ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½ PromptTemplate â€”â€”
PROMPT_PATH = Path("app/prompts/emotion_prompt_en.txt")
with PROMPT_PATH.open("r", encoding="utf-8") as f:
    prompt_str = f.read()

prompt_template = PromptTemplate(
    input_variables=["breed", "emotion"],
    template=prompt_str
)

# â€”â€” åå¤„ç†å‡½æ•°ï¼šå»æ‰æ€è€ƒå†…å®¹ï¼Œåªä¿ç•™æœ€ç»ˆå›ç­” â€”â€” 
def strip_thoughts(text: str) -> str:
    # åˆ é™¤ <think>â€¦</think> åŒºå—
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.S)
    # åˆ é™¤ä»¥â€œæ€è€ƒâ€å¼€å¤´çš„è¡Œ
    text = re.sub(r"^æ€è€ƒ[^\n]*\n?", "", text, flags=re.M)
    return text.strip()

# âœ… ç”Ÿæˆç‹—ç‹—æƒ…ç»ªå»ºè®®ï¼ˆå«åå¤„ç†ï¼‰
def generate_emotion_advice(breed: str, emotion: str) -> str:
    chain = LLMChain(llm=llm, prompt=prompt_template)
    raw_output = chain.run(breed=breed, emotion=emotion)
    clean_output = strip_thoughts(raw_output)
    return clean_output

# âœ… æä¾›ç»™ LangChain QA ç”¨çš„ LLM æ¥å£
def get_local_llm() -> HuggingFacePipeline:
    return llm




"""
import os
# â€”â€” å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œé˜²æ­¢è¯¯è¿ Hugging Face â€”â€” 
os.environ["TRANSFORMERS_OFFLINE"] = "1"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline

device = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = "/app/LLM_Models/Lora+_DeepSeek_R1_Distill_Qwen_1_5B"

print("å¼€å§‹åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, trust_remote_code=True, local_files_only=True
)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
).eval()

# æƒ…ç»ªæ˜ å°„ï¼šè‹±æ–‡->ä¸­æ–‡
emotion_map = {
    "happy": "å¼€å¿ƒ",
    "sad": "éš¾è¿‡",
    "angry": "æ„¤æ€’",
    "anxious": "ç„¦è™‘",
    "relaxed": "æ”¾æ¾",
}

def normalize_breed(breed: str) -> str:
    return " ".join(w.capitalize() for w in breed.split("_"))

# PromptTemplateï¼šåªç”Ÿæˆå»ºè®®æ­£æ–‡ï¼Œä¸è´Ÿè´£æ£€æµ‹æç¤º
template = PromptTemplate.from_template(
"""
#System: ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å® ç‰©å¿ƒç†ä¸“å®¶ï¼Œåªè¾“å‡ºæœ€ç»ˆå»ºè®®ï¼Œ**ç»ä¸**è¾“å‡ºä»»ä½•æ€è€ƒã€åˆ†æè¿‡ç¨‹æˆ–è¿‡æ¸¡çŸ­è¯­ï¼Œç¦æ­¢ä½¿ç”¨â€œå¥½çš„â€ã€â€œæˆ‘ç°åœ¨éœ€è¦â€¦â€ã€â€œæ¥ä¸‹æ¥â€ã€â€œç„¶åâ€ã€â€œé¦–å…ˆâ€ç­‰ã€‚

#User:
#è¯·æ ¹æ®çŠ¬ç§â€œ{breed_name_en}â€å’Œå½“å‰æƒ…ç»ªâ€œ{emotion_cn}â€ï¼Œç”¨ä¸­æ–‡å†™ä¸€æ®µ 200â€“300 å­—ã€æµç•…è‡ªç„¶çš„å® ç‰©å¿ƒç†å»ºè®®ï¼Œå†…å®¹æ¶µç›–å¯èƒ½åŸå› ã€å®‰æŠšæ–¹æ³•å’Œé¢„é˜²ç­–ç•¥ï¼Œå¯é€‚å½“ä½¿ç”¨ğŸ˜Šã€ğŸ¾ç­‰è¡¨æƒ…å¢æ·»äº²å’ŒåŠ›ï¼Œä½†ä¸è¦è¿‡åº¦ã€‚è¯·ç›´æ¥ç»™å‡ºå»ºè®®æ­£æ–‡ï¼Œä¸è¦ç¼–å·æˆ–åˆ†ç‚¹ï¼Œä¹Ÿä¸è¦ä»»ä½•å‰è¨€ã€‚

#Answer:
""")

# æ„å»ºæ–‡æœ¬ç”Ÿæˆ pipelineï¼ˆä¸ä½¿ç”¨åœæ­¢ç­–ç•¥ä¸ç¦ç”¨è¯ï¼‰
text_gen = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    max_new_tokens=256,    # â† è¿™é‡ŒæŒ‡å®šé•¿åº¦
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
)

def generate_emotion_advice(breed: str, emotion: str) -> str:
    # è§„èŒƒåŒ–å“ç§ä¸æƒ…ç»ªæè¿°
    breed_clean = normalize_breed(breed)
    emotion_cn = emotion_map.get(emotion.lower(), emotion)
    # â€œæ€è€ƒâ€éƒ¨åˆ†ï¼šæ£€æµ‹æç¤ºï¼ˆæ™®é€šå­—ä½“ï¼‰
    thinking = f"ğŸ¶ æˆ‘æ£€æµ‹åˆ°è¿™æ˜¯ä¸€åª {breed_clean}ï¼Œå®ƒç°åœ¨å¯èƒ½æ„Ÿåˆ° {emotion_cn}ã€‚\n\n"
    # æ¨¡æ¿ prompt ä»…ç”Ÿäº§å»ºè®®æ­£æ–‡
    prompt = template.format(breed_name_en=breed_clean, emotion_cn=emotion_cn)
    # ç”Ÿæˆæ­£æ–‡
    output = text_gen(prompt, return_full_text=False)[0]["generated_text"].strip()
    # â€œæ­£å¼å†…å®¹â€éƒ¨åˆ†åŠ ç²—
    answer = f"**{output}**"
    return thinking + answer

def get_local_llm() -> HuggingFacePipeline:
    llm_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        device_map="auto"
    )
    return HuggingFacePipeline(pipeline=llm_pipe)

# â€”â€” ç¤ºä¾‹è°ƒç”¨ â€”â€” 
if __name__ == "__main__":
    advice = generate_emotion_advice("berner_mountain_dog", "happy")
    print(advice)
"""