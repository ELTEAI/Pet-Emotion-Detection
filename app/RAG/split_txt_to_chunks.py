# app/RAG/split_txt_to_chunks.py

import os
import glob
from tqdm import tqdm
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from app.RAG.embedding_utils import load_embedding_model, embed_texts
from app.RAG.config      import WIKI_DOGS_PATH, CHROMA_DB_PATH, EMBEDDING_MODEL_PATH

def main():
    # 1. éªŒè¯è·¯å¾„
    if not os.path.isdir(WIKI_DOGS_PATH):
        raise FileNotFoundError(f"WIKI_DOGS_PATH ä¸å­˜åœ¨: {WIKI_DOGS_PATH}")
    os.makedirs(CHROMA_DB_PATH, exist_ok=True)

    # 2. è¯»å–æ‰€æœ‰ txt æ–‡ä»¶
    txt_files = glob.glob(os.path.join(WIKI_DOGS_PATH, "*.txt"))
    print(f"ğŸ“„ æ‰¾åˆ° {len(txt_files)} ä¸ªæè¿°æ–‡ä»¶")
    docs = []
    for fn in tqdm(txt_files, desc="è¯»å–æ–‡ä»¶"):
        with open(fn, "r", encoding="utf-8") as f:
            docs.append({"text": f.read(), "source": os.path.basename(fn)})

    # 3. åˆ‡åˆ†æˆ chunk
    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
    chunks, metadatas = [], []
    for doc in tqdm(docs, desc="åˆ‡åˆ†æ–‡æœ¬"):
        segs = splitter.split_text(doc["text"])
        chunks.extend(segs)
        metadatas.extend([{"source": doc["source"]}] * len(segs))
    print(f"ğŸ§© æ€»å…±åˆ‡åˆ†å‡º {len(chunks)} ä¸ª chunk")

    # 4. åŠ è½½æœ¬åœ° HuggingFace åµŒå…¥æ¨¡å‹
    print(f"ğŸ¤– æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL_PATH}")
    embed_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)

    # 5. å®ä¾‹åŒ– Chroma å‘é‡åº“ wrapper
    vectordb = Chroma(
        embedding_function=embed_model,
        persist_directory=CHROMA_DB_PATH,
        collection_name="dog_wiki"
    )

    # 6. åˆ†æ‰¹å†™å…¥ï¼Œé¿å…è¶…è¿‡åº•å±‚ upsert é™åˆ¶
    ids = [str(i) for i in range(len(chunks))]
    batch_size = 5000  # å°äºåº•å±‚æœ€å¤§ 5461
    print(f"ğŸ”„ åˆ†æ‰¹ (batch_size={batch_size}) å†™å…¥å‘é‡åº“â€¦")
    for start in tqdm(range(0, len(chunks), batch_size), desc="å†™å…¥æ‰¹æ¬¡"):
        end = min(start + batch_size, len(chunks))
        vectordb.add_texts(
            texts=chunks[start:end],
            metadatas=metadatas[start:end],
            ids=ids[start:end],
        )

    # 7. Persist
    vectordb.persist()
    print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼Œä¿å­˜åœ¨", CHROMA_DB_PATH)


if __name__ == "__main__":
    main()
