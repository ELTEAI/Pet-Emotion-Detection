import os
import redis
import torch
import streamlit as st
from pathlib import Path
from streamlit_chat import message
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline
)
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from langchain.chains import LLMChain
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from model_utils import load_emotion_model, predict_emotion
from dog_breed_utils import predict_breed
from RAG.config import CHROMA_DB_PATH, EMBEDDING_MODEL_PATH

# â€”â€” å¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼ â€”â€”
os.environ["TRANSFORMERS_OFFLINE"] = "1"

# â€”â€” é¡µé¢é…ç½® â€”â€” #
st.set_page_config(page_title="ğŸ¶ Dog Emotions + Chat Assistant", layout="centered")

# â€”â€” Redis è®¿é—®ç»Ÿè®¡ï¼ˆå¸¦è¶…æ—¶å’Œå¤‡ç”¨æ–¹æ¡ˆï¼‰â€”â€” #
try:
    r = redis.Redis(
        host="dog-ai-redis",
        port=6379,
        db=0,
        socket_timeout=3,  # 3ç§’è¶…æ—¶
        socket_connect_timeout=3
    )
    visits = r.incr("visit_count")
except redis.exceptions.ConnectionError:
    # Redisè¿æ¥å¤±è´¥æ—¶ä½¿ç”¨ä¼šè¯çŠ¶æ€ä½œä¸ºåå¤‡
    if "visit_count" not in st.session_state:
        st.session_state.visit_count = 0
    st.session_state.visit_count += 1
    visits = st.session_state.visit_count
    st.sidebar.warning("âš ï¸ Redis connection failed, use local access statistics")
except Exception as e:
    visits = "Statistics not available"
    st.sidebar.error(f"âŒ Access statistics error: {str(e)}")

# â€”â€” å±•ç¤ºè®¿é—®æ¬¡æ•° â€”â€” #
st.sidebar.markdown(f"ğŸ‘€ **Number of visitsï¼š{visits}**")

# â€”â€” RAG é…ç½® â€”â€” #
#CHROMA_DB_PATH = "path/to/your/chroma_db"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
#EMBEDDING_MODEL_PATH = "path/to/your/embedding_model"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„

# â€”â€” ç¼“å­˜èµ„æºåŠ è½½ â€”â€” #
@st.cache_resource
def load_embedding_model():
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)

@st.cache_resource
def load_vector_db():
    return Chroma(
        persist_directory=CHROMA_DB_PATH,
        embedding_function=load_embedding_model(),
        collection_name="dog_wiki"
    )

# â€”â€” æ¨¡å‹åˆ‡æ¢é€»è¾‘ â€”â€” #
if "model_choice" not in st.session_state:
    st.session_state.model_choice = "OpenAI API"

model_choice = st.sidebar.selectbox(
    "ğŸ¤– Choosing a language model",
    ["OpenAI API", "æœ¬åœ° Lora DeepSeek", "Qwen3 API"],
    index=["OpenAI API", "æœ¬åœ° Lora DeepSeek", "Qwen3 API"].index(st.session_state.model_choice)
)

# æ¨¡å‹æ˜ å°„
model_map = {
    "OpenAI API": "openai",
    "æœ¬åœ° Lora DeepSeek": "local",
    "Qwen3 API": "qwen3"
}

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆ‡æ¢
if model_choice != st.session_state.model_choice:
    st.session_state.model_choice = model_choice
    st.rerun()

current_model = model_map[model_choice]

# â€”â€” åˆå§‹åŒ–ç»Ÿä¸€ä¼šè¯çŠ¶æ€ â€”â€” #
if "app_state" not in st.session_state:
    st.session_state.app_state = {
        "messages": [],
        "breed": None,
        "emotion": None,
        "image_processed": False
    }

state = st.session_state.app_state

# â€”â€” æ ‡é¢˜ â€”â€” #
st.title(f"ğŸ¾ Dog emotion recognition and intelligent chat assistant ({model_choice})")

# â€”â€” 1. ä¸Šä¼ å›¾ç‰‡ & è¯†åˆ« â€”â€” #
uploaded_file = st.file_uploader(
    "ğŸ“¤ Upload a photo of your dog for analysis", type=["jpg", "jpeg", "png"]
)

if uploaded_file:
    # é‡ç½®å›¾ç‰‡å¤„ç†çŠ¶æ€
    state["image_processed"] = False
    state["breed"] = None
    state["emotion"] = None
    
    # ä¿å­˜å¹¶å±•ç¤ºå›¾ç‰‡
    upload_dir = os.path.join("app", "uploads")
    os.makedirs(upload_dir, exist_ok=True)
    img_path = os.path.join(upload_dir, uploaded_file.name)
    with open(img_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    st.image(img_path, caption="Photos of your dog that you uploaded", width=400)

    # å¸¦çŠ¶æ€æŒ‡ç¤ºå™¨çš„è¯†åˆ«è¿‡ç¨‹
    with st.status("ğŸ” Analyzing dog photos...", expanded=True) as status:
        st.write("Identify the species...")
        breed = predict_breed(img_path)
        st.write("Analyzing emotions...")
        emotion = predict_emotion(img_path, load_emotion_model())
        
        # æ›´æ–°çŠ¶æ€
        state["breed"] = breed
        state["emotion"] = emotion
        
        status.update(label="âœ… Analysis Complete!", state="complete")
    
    st.success(f"ğŸ” Identification result: Breed - **{breed}**ï¼›Emotion - **{emotion}**")

    # ä»…é¦–æ¬¡å¤„ç†å›¾ç‰‡æ—¶æ‰ç”Ÿæˆå»ºè®®
    if not state["image_processed"]:
        # åŠ¨æ€å¯¼å…¥ç›¸åº”æ¨¡å‹çš„å‡½æ•°
        if current_model == "openai":
            from Open_AI_API.langchain_utils import generate_emotion_advice
        elif current_model == "local":
            from lora_deepseek_r1_distill_qwen_1_5b.langchain_utils_lora_deepseek_r1_distill_qwen_1_5b import generate_emotion_advice
        elif current_model == "qwen3":
            from qwen3.langchain_utils_qwen3 import generate_emotion_advice
        
        with st.spinner("ğŸ’¡ Generating sentiment suggestions..."):
            advice = generate_emotion_advice(breed, emotion)
            bot_msg = (
                f"ğŸ¶ I detected this is a**{breed}**ï¼Œ"
                f"It may now feel**{emotion}**ã€‚\n\nğŸ‘‰ suggestionï¼š{advice}"
            )
            state["messages"].append({
                "role": "assistant", "content": bot_msg
            })
            state["image_processed"] = True

# â€”â€” ç©ºçŠ¶æ€å¤„ç† â€”â€” #
if not state["messages"]:
    st.info("ğŸ‘‹ Upload a photo of your dog to start analysis, or ask a question below")

# â€”â€” 2. èŠå¤©å¼é—®ç­” â€”â€” #
query = st.chat_input("Please enter a question, for example: Is Samoyed afraid of heat?")

if query:
    # æ›´æ™ºèƒ½çš„å“ç§ä¸Šä¸‹æ–‡æ·»åŠ 
    if state["breed"]:
        breed = state["breed"]
        # æ£€æŸ¥å“ç§æ˜¯å¦å·²åœ¨é—®é¢˜ä¸­æåˆ°
        breed_mentioned = any([
            breed in query,
            breed.split()[0] in query,  # å¤„ç†å¤šè¯å“ç§å
            breed.replace(" ", "") in query.replace(" ", "")
        ])
        
        if not breed_mentioned:
            query = f"About{breed}Breed issuesï¼š" + query
    
    # è¿½åŠ ç”¨æˆ·è¾“å…¥
    state["messages"].append({"role": "user", "content": query})
    
    # åŠ¨æ€å¯¼å…¥ç›¸åº”æ¨¡å‹çš„å‡½æ•°
    if current_model == "openai":
        from Open_AI_API.langchain_utils import get_openai_llm
        get_llm = get_openai_llm
    elif current_model == "local":
        from lora_deepseek_r1_distill_qwen_1_5b.langchain_utils_lora_deepseek_r1_distill_qwen_1_5b import get_local_llm
        get_llm = get_local_llm
    elif current_model == "qwen3":
        from qwen3.langchain_utils_qwen3 import get_qwen3_llm
        get_llm = get_qwen3_llm

    # RAG æ£€ç´¢ + LLM å›ç­”
    with st.spinner("ğŸ¤” Thinking..."):
        try:
            # ä½¿ç”¨ç¼“å­˜çš„å‘é‡æ•°æ®åº“
            vectordb = load_vector_db()
            retriever = vectordb.as_retriever(search_kwargs={"k": 5})
            llm = get_llm()
            qa = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=False
            )
            answer = qa.run(query)
        except Exception as e:
            st.error(f"Error answering question: {str(e)}")
            answer = "Sorry, there was an error answering the question, please try again laterã€‚"

    # è¿½åŠ æœºå™¨äººå›ç­”
    state["messages"].append({"role": "assistant", "content": answer})

# â€”â€” 3. ä¼˜åŒ–åçš„å¯¹è¯æ¸²æŸ“ â€”â€” #
if state["messages"]:
    st.markdown("---")
    st.subheader("ğŸ’¬ Conversation history")
    
    # ä»…æ¸²æŸ“æœ€å10æ¡æ¶ˆæ¯
    for msg in state["messages"][-10:]:
        message(
            msg["content"],
            is_user=(msg["role"] == "user"),
            key=f"{id(msg)}"  # ä½¿ç”¨æ¶ˆæ¯å†…å®¹çš„IDä½œä¸ºkey
        )