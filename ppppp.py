# app/main.py
""""""
import redis

import os
import streamlit as st
from streamlit_chat import message

# â€”â€” é¡µé¢é…ç½® â€”â€” #
st.set_page_config(page_title="ğŸ¶ Dog Emotions + Chat Assistant", layout="centered")
# â€”â€” è®¿é—®ç»Ÿè®¡ â€”â€” #
try:
    r = redis.Redis(host="dog-ai-redis", port=6379, db=0)
    visits = r.incr("visit_count")
except Exception as e:
    visits = "âŒ Redis è¿æ¥å¤±è´¥"
    print(f"[è®¿é—®è®¡æ•°é”™è¯¯] {e}")

# â€”â€” å±•ç¤ºè®¿é—®æ¬¡æ•° â€”â€” #
st.sidebar.markdown(f"ğŸ‘€ **Number of visitsï¼š{visits}**")


# â€”â€” å¯¼å…¥æ¨¡å‹ä¸å·¥å…· â€”â€” #
from model_utils import load_emotion_model, predict_emotion
from dog_breed_utils import predict_breed
from RAG.config import CHROMA_DB_PATH, EMBEDDING_MODEL_PATH
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# â€”â€” è·å–å½“å‰é¡µé¢å‚æ•° â€”â€” #
query_params = st.experimental_get_query_params()
#query_params = st.query_params
current_model = query_params.get("model", ["openai"])[0]

# â€”â€” ä¾§è¾¹æ ï¼šåˆ‡æ¢æ¨¡å‹ â€”â€” #
model_choice = st.sidebar.selectbox(
    "ğŸ¤– Choosing a language model",
    ["OpenAI API", "æœ¬åœ° Lora DeepSeek", "Qwen3 API"],
    index=0 if current_model == "openai" else 1 if current_model == "local" else 2
)
# æ—§ä»£ç ï¼šä½¿ç”¨ st.experimental_set_query_params
# å½“æ¨¡å‹é€‰æ‹©æ”¹å˜æ—¶ï¼Œæ›´æ–° URL å‚æ•°
if model_choice == "OpenAI API" and current_model != "openai":
    st.experimental_set_query_params(model="openai")
    #st.query_params = {"model": "openai"}
    st.rerun()
elif model_choice == "æœ¬åœ° Lora DeepSeek" and current_model != "local":
    st.experimental_set_query_params(model="local")
    #st.query_params = {"model": "local"}
    st.rerun()
elif model_choice == "Qwen3 API" and current_model != "qwen3":
    st.experimental_set_query_params(model="qwen3"); 
    #st.query_params = {"model": "qwen3"}
    st.rerun()
# æ ¹æ®å½“å‰æ¨¡å‹å¯¼å…¥ç›¸åº”çš„å·¥å…·
if current_model == "openai":
    from Open_AI_API.langchain_utils import generate_emotion_advice, get_openai_llm
    get_llm = get_openai_llm
elif current_model == "local":
    from lora_deepseek_r1_distill_qwen_1_5b.langchain_utils_lora_deepseek_r1_distill_qwen_1_5b import (
        generate_emotion_advice, get_local_llm
    )
    get_llm = get_local_llm
elif current_model == "qwen3":
    from qwen3.langchain_utils_qwen3 import generate_emotion_advice, get_qwen3_llm
    get_llm = get_qwen3_llm

# â€”â€” åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ â€”â€” #
model_prefix = "openai_" if current_model == "openai" else "local_"
if f"{model_prefix}messages" not in st.session_state:
    st.session_state[f"{model_prefix}messages"] = []
if f"{model_prefix}breed" not in st.session_state:
    st.session_state[f"{model_prefix}breed"] = None
if f"{model_prefix}emotion" not in st.session_state:
    st.session_state[f"{model_prefix}emotion"] = None
if f"{model_prefix}image_processed" not in st.session_state:
    st.session_state[f"{model_prefix}image_processed"] = False

# â€”â€” æ ‡é¢˜ â€”â€” #
st.title(f"ğŸ¾ Dog Emotion Recognition & Smart Chat Assistant ({model_choice})")

# â€”â€” 1. ä¸Šä¼ å›¾ç‰‡ & è¯†åˆ« â€”â€” #
uploaded_file = st.file_uploader(
    "ğŸ“¤ Upload a photo of your dog for analysis", type=["jpg", "jpeg", "png"]
)
if uploaded_file:
    # ä¿å­˜å¹¶å±•ç¤ºå›¾ç‰‡
    upload_dir = os.path.join("app", "uploads")
    os.makedirs(upload_dir, exist_ok=True)
    img_path = os.path.join(upload_dir, uploaded_file.name)
    with open(img_path, "wb") as f:
        f.write(uploaded_file.read())
    st.image(img_path, caption="ä½ ä¸Šä¼ çš„ç‹—ç‹—ç…§ç‰‡", width=400)

    # è¯†åˆ«å“ç§ä¸æƒ…ç»ª
    with st.spinner("ğŸ• Identifyingâ€¦"):
        breed = predict_breed(img_path)
        emotion = predict_emotion(img_path, load_emotion_model())
        # æ›´æ–°çŠ¶æ€
        st.session_state[f"{model_prefix}breed"] = breed
        st.session_state[f"{model_prefix}emotion"] = emotion

    st.success(f"ğŸ” Identification result: Breed - **{breed}**ï¼›Emotion: - **{emotion}**")

    # ä»…é¦–æ¬¡å¤„ç†å›¾ç‰‡æ—¶æ‰ç”Ÿæˆå»ºè®®
    if not st.session_state[f"{model_prefix}image_processed"]:
        advice = generate_emotion_advice(breed, emotion)
        bot_msg = (
            f"ğŸ¶ I detected that this was a **{breed}**ï¼Œ"
            f"It may now feel **{emotion}**ã€‚\n\nğŸ‘‰ å»ºè®®ï¼š{advice}"
        )
        st.session_state[f"{model_prefix}messages"].append({
            "role": "assistant", "content": bot_msg
        })
        st.session_state[f"{model_prefix}image_processed"] = True

# â€”â€” 2. èŠå¤©å¼é—®ç­” â€”â€” #
query = st.chat_input("Please enter a question, for example: Are Samoyeds afraid of heat?")
if query:
    breed = st.session_state[f"{model_prefix}breed"]
    # è‡ªåŠ¨è¡¥å…¨å“ç§ä¸Šä¸‹æ–‡
    if breed and breed not in query:
        query = f"{breed}is dog`s breed,{query}"

    # è¿½åŠ ç”¨æˆ·è¾“å…¥
    st.session_state[f"{model_prefix}messages"].append({"role": "user", "content": query})

    # RAG æ£€ç´¢ + LLM å›ç­”
    with st.spinner("ğŸ¤” Answering..."):
        embed_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)
        vectordb = Chroma(
            persist_directory=CHROMA_DB_PATH,
            embedding_function=embed_model,
            collection_name="dog_wiki"
        )
        retriever = vectordb.as_retriever(search_kwargs={"k": 5})
        llm = get_llm()
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False
        )
        answer = qa.run(query)

    # è¿½åŠ æœºå™¨äººå›ç­”
    st.session_state[f"{model_prefix}messages"].append({"role": "assistant", "content": answer})

# â€”â€” 3. ç»Ÿä¸€æ¸²æŸ“å¯¹è¯ â€”â€” #
if st.session_state[f"{model_prefix}messages"]:
    st.markdown("---")
    st.subheader("ğŸ’¬ Chat History")
    for idx, msg in enumerate(st.session_state[f"{model_prefix}messages"]):
        message(
            msg["content"],
            is_user=(msg["role"] == "user"),
            key=f"{model_prefix}msg_{idx}"
        )  ä»£ç ä¸­CloudflareåŠŸèƒ½æ˜¯å“ªä¸ª 